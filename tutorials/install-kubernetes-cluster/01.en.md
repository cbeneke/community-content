---
SPDX-License-Identifier: MIT
path: "/tutorials/install-kubernetes-cluster"
slug: "install-kubernetes-cluster"
date: "2019-05-04"
title: "Install a Kubernetes cluster on cloud servers"
short_description: "Guide to install and set up a Kubernetes cluster supporting the full range of Kubernetes objects on Hetzner Cloud servers."
tags: ["Kubernetes"]
author: "Christian Beneke"
author_link: "https://github.com/cbeneke"
author_img: "https://avatars3.githubusercontent.com/u/222009"
author_description: ""
language: "en"
available_languages: ["en"]

---

## Introduction

This tutorial will guide you through the setup of a [Kubernetes](https://kubernetes.io) cluster on Hetnzer Cloud servers. The resulting cluster will support the full range of Kubernetes objects including [LoadBalancer service types](https://kubernetes.io/docs/concepts/services-networking/service/#loadbalancer),
[persistent volumes](https://kubernetes.io/docs/concepts/storage/persistent-volumes/) and a private network between the servers. It will not cover high availability of the Kubernetes control plane.

### Prerequisites

* A [Hetzner Cloud](https://www.hetzner.com/cloud) account
* Familiarity with the [concepts of Kubernetes](https://kubernetes.io/docs/concepts/)
* Familiarity with linux and working on the shell
* `ssh`, `hcloud`, `kubectl` and `helm` command line tools installed

*This tutorial was tested on Ubuntu 18.04 Hetzner Cloud servers and Kubernetes version v1.14.1*

### Terminology and Notation

*Commands*

```bash
local$ <command>  # This command must be executed on your local machine
all$ <command>   # This command must be executed on all servers as root
master$ <command> # This command must be executed on the master server as root
worker$ <command> # This command must be executed on all worker servers as root
```

Multiline commands end with a backslash. E.g.

```bash
local$ command --short-option \
  --with-quite-long-parameter=9001
```

---

*Files*

A to be configured file will be described as `/path/to/file.txt`

```config
And the full content of the file follwing in the box after the sentence
```

---

*IP Addresses*

* `<10.0.0.X>` internal IP Address
* `<116.203.0.X>` public IP Address
* `<159.69.0.1>`public floating IP Address

---

*Explanations*

> This is a deeper explanation to the previous content. It might contain useful information, but if you did understand everything it can safely be skipped.

## Step 1 - Create new Hetzner Cloud resources

For this tutorial the following resources will be used

* 1 Hetzner cloud CX11 server
* 2 Hetzner cloud CX21 servers
* 1 IPv4 floating IP address
* 1 IPv6 floating IP address (optional)

The CX11 server will be the master node and the CX21 servers the worker nodes. The floating IP address(es) will be used for LoadBalancer services.

Create the required resources in the web interface or with the hcloud CLI tool

```bash
local$ hcloud server create --type cx11 --name master-1 --image ubuntu-18.04 --ssh-key <ssh_key_id>
local$ hcloud server create --type cx21 --name worker-1 --image ubuntu-18.04 --ssh-key <ssh_key_id>
local$ hcloud server create --type cx21 --name worker-2 --image ubuntu-18.04 --ssh-key <ssh_key_id>
local$ hcloud floating-ip create --type ipv4 --home-location nbg1
local$ hcloud floating-ip create --type ipv6 --home-location nbg1  # (Optional)
```

> The existing ssh keys can be listed with `hcloud ssh-key list`.

The names of the server do not affect the cluster creation, but should not be changed later on. Feel free to change the image and type regarding your needs.

### Step 1.1 - Update the servers (Optional)

It is recommended to update the servers after creation. Log onto each server and run

```bash
all$ apt-get update
all$ apt-get dist-upgrade
all$ reboot
```

## Step 2 - Configure the network

Hetzner Cloud does not support private network(s) between the cloud servers. For improved security it is recommend to set up a virtual private network (VPN) between the servers.

> If no VPN is set up between the servers they will communicate exclusively over the public IP addresses, which means the encryption of the traffic depends on the application.

Each server will have a private and a public IP address configured. Those are noted as `<10.0.0.1>` as private and `<116.203.0.1>` as public IP address. Please adapt to your needs (e.g. if the servers are not located in a VPN) and make sure to not reuse any IP address.

### Step 2.1 - Span a private network (Optional)

[Wireguard](https://www.wireguard.com/) will be used as VPN provider between the servers. Install the required packages on all servers

```bash
all$ add-apt-repository ppa:wireguard/wireguard
all$ apt-get install wireguard linux-headers-generic
```

When the installation was successful, create a wireguard keypair on each server. It will be used to encrypt the traffic between the servers

```bash
all$ wg genkey | tee /etc/wireguard/wg0-privatekey \
        | wg pubkey > /etc/wireguard/wg0-publickey
all$ chmod 400 /etc/wireguard/wg0-privatekey
```

Then create the wireguard configuration file `/etc/wireguard/wg0.conf`

```config
[Interface]
PrivateKey = <privatekey_1>
Address = <10.0.0.1>
ListenPort = 51820
SaveConfig = false

[Peer]
PublicKey = <publickey_2>
Endpoint = <116.203.0.2>:51820
AllowedIPs = <10.0.0.2>/32

[Peer]
PublicKey = <publickey_3>
Endpoint = <116.203.0.3>:51820
AllowedIPs = <10.0.0.3>/32
```

> `<privatekey_1>` is the content of `/etc/wireguard/wg0-privatekey` on the first node, `<publickey_2>` the content of `/etc/wireguard/wg0-publickey` on the second node, etc.

> The keys are a base64 encoded string. E.g. `8ZRjKffP0ZBj7L1iPoY4mgrseT37QhHwrR6bqH9jgVY=`

> Each node needs to be configured with the `[Interface]` section setting up its own privatekey and IP address and both `[Peer]` sections the respective publickeys and IP address of the peer server.

Wireguard should now be able to be started on each server

```bash
all$ systemctl start wg-quick@wg0.service
```

When launched successfully the other nodes should be reachable over their internal IP addresses

```bash
all$ ping -c 1 10.0.0.1
PING 10.0.0.1 (10.0.0.1) 56(84) bytes of data.
64 bytes from 10.0.0.1: icmp_seq=1 ttl=64 time=0.591 ms

all$ ping -c 1 10.0.0.3
PING 10.0.0.3 (10.0.0.3) 56(84) bytes of data.
64 bytes from 10.0.0.3: icmp_seq=1 ttl=64 time=1.28 ms
```

As last step add a label on all nodes to allow the automated use of the internal IP address later on. Therefor run the following command on your local machine for each server

```bash
local$ hcloud server add-label <server_name> kubernetes.io/internal-ip=<10.0.0.1>
```

Make sure to label exactly the IP address you configured earlier.

### Step 2.2 - Configure floating IPs

For the LoadBalancer service type a floating IPs was registered  in the first step. This IP address will be noted as `<159.69.0.1>`. For the setup to actually work, the IP address needs to be configured on all worker nodes. Create a file `/etc/network/interfaces.d/60-floating-ip.cfg`

```config
auto eth0:1
iface eth0:1 inet static
  address <159.69.0.1>
  netmask 32
```

If you also registered a IPv6 floating IP extend the config by

```config
iface eth0:1 inet6 static
  address <2001:db8:1234::1>
  netmask 64
```

Then restart your networking

```bash
all$ systemctl restart networking.service
```

These IP addresses should not yet be assigned to a server, so don't worry that they can't be reached for now.

### Step 2.3 - Configure IPv6 to IPv4 translation (Optional)

If you have registered a IPv6 floating IP, the worker nodes need to be configured to translate IPv6 to IPv4, as Kubernetes currently does not support a IPv4/6 dual stack. This tutorial will (ab)use [gobetween](http://gobetween.io/) for the translation.

> Gobetween is a lightweight Layer 4 loadbalancer, that supports both TCP and UDP. Any other service which translates IPv4 to IPv6 can be used instead aswell.

To install gobetween download and extract the latest version from their [GitHub repository](https://github.com/yyyar/gobetween/releases) and place it in `/usr/local/bin/`. As of writing this guide, the current version is 0.7.0

```bash
all$ cd /usr/local/bin
all$ wget https://github.com/yyyar/gobetween/releases/download/0.7.0/gobetween_0.7.0_linux_amd64.tar.gz
all$ tar -xvf gobetween_0.7.0_linux_amd64.tar.gz gobetween
all$ rm gobetween_0.7.0_linux_amd64.tar.gz
```

Then create a small systemd unit file `/etc/systemd/system/gobetween.service`

```config
[Unit]
Description=Gobetween - modern LB for cloud era
Documentation=https://github.com/yyyar/gobetween/wiki
After=network.target remote-fs.target nss-lookup.target

[Service]
Type=simple
PIDFile=/run/gobetween.pid
ExecStart=/usr/local/bin/gobetween from-file /etc/gobetween.json -f json
ExecReload=/bin/kill -s HUP $MAINPID
ExecStop=/bin/kill -s QUIT $MAINPID
PrivateTmp=true

[Install]
WantedBy=multi-user.target
```

And a configuration file `/etc/gobetween.json`

```config
{
  "api": {
    "enabled": false
  },
  "logging": {
    "level": "info",
    "output": "stdout"
  },
  "servers": {
    "IPv6-tcp-http": {
      "bind": "[<2001:db8:1234::1>]:80",
      "protocol": "tcp",
      "discovery": {
        "kind": "static",
        "static_list": [
          "<159.69.0.1>:80"
        ]
      }
    },
    "IPv6-tcp-https": {
      "bind": "[<2001:db8:1234::1>]:443",
      "protocol": "tcp",
      "discovery": {
        "kind": "static",
        "static_list": [
          "<159.69.0.1>:443"
        ]
      }
    }
  }
}
```

This config file should be adapted to match your needs. Check out the [gobetween documentation](http://gobetween.io/documentation.html) for further information.

> The basic idea is to listen on each used port and protocol on the IPv6 floating IP address and forward it to the IPv4 floating IP address.

When the configuration suits your needs you can reload the systemd unit files and start the gobetween service

```bash
all$ systemctl daemon-reload
all$ systemctl start gobetween.service
```

## Step 3 - Install Kubernetes

To install the Kubernetes cluster on the servers we will utilise [kubeadm](https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/). The interface between Kubernetes and the Hetzner Cloud will be (a patched version of) the [Hetzner Cloud Controller Manager](https://github.com/hetznercloud/hcloud-cloud-controller-manager) and the [Hetzner Cloud Container Stroage Interface](https://github.com/hetznercloud/csi-driver). Both tools are provided by the Hetzner Cloud team.

### Step 3.1 - Prepare the Cloud Controller Manager

The [Hetzner Cloud Controller Manager](https://github.com/hetznercloud/hcloud-cloud-controller-manager) requires the Kubernetes cluster to be set up to use an external cloud provider. Therefor create a file `/etc/systemd/system/kubelet.service.d/20-hetzner-cloud.conf` on each server

```config
[Service]
Environment="KUBELET_EXTRA_ARGS=--cloud-provider=external"
```

> This will make sure, that the kubelet is started with the `cloud-provider = external` flag. Any further configuration will be handled by kubeadm later on.

### Step 3.2 - Install Docker and Kubernetes Packages

As Docker and Kubernetes are installed on a distribution with systemd as init system, Docker should be set up to use the systemd cgroups. To do so create a file `/etc/systemd/system/docker.service.d/00-cgroup-systemd.conf` on each server

```config
[Service]
ExecStart=
ExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock --exec-opt native.cgroupdriver=systemd
```

If any further flags are required for Docker expand the ExecStart option accordingly. Then reload the systemd unit files on all servers to use the configured options

```bash
all$ systemctl daemon-reload
```

Then install the required packages run the following commands on all servers

```bash
all$ curl -fsSL https://download.docker.com/linux/ubuntu/gpg | apt-key add -
all$ curl -fsSL https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -
all$ cat <<EOF >/etc/apt/sources.list.d/docker-and-kubernetes.list
        deb [arch=amd64] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable
        deb http://packages.cloud.google.com/apt/ kubernetes-xenial main
EOF
all$ apt-get update
all$ apt-get install docker-ce kubeadm kubectl kubelet
```

You need to make sure that the system can actually forward traffic between the nodes and pods. Set the following sysctl settings on each server

```bash
all$ cat <<EOF >>/etc/sysctl.conf

# Allow IP forwarding for kubernetes
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_forward = 1
net.ipv6.conf.default.forwarding = 1
EOF
all$ sysctl -p
```

> These settings will allow forwarding of IPv4 and IPv6 packages between multiple network interfaces. This is required because each container has its own virtual network interface.

### Step 3.3 - Setup control plane

The servers are now prepared to finally install the Kubernetes cluster. Log on to the master node and initialize the cluster

```bash
master$ kubeadm config images pull
master$ kubeadm init \
  --pod-network-cidr=10.244.0.0/16 \
  --kubernetes-version=v1.14.1 \
  --ignore-preflight-errors=NumCPU \
  --apiserver-cert-extra-sans <10.0.0.1>
```

The `kubeadm init` process will print a `kubeadm join` command inbetween. You should copy that command for later use (not required as you can always create a new token when needed). The `--apiserver-cert-extra-sans` flag is only required if you set up an internal network.

> If the server has a flavor bigger than CX11, the `ignore-preflight-errors` flag does not need to be passed. You can add additional flags regarding your needs. Check the [documentation](https://kubernetes.io/docs/reference/setup-tools/kubeadm/kubeadm-init/#options) for further information.

When the initialisation is complete begin with setting up the required master components in the cluster. For ease of use configure the kubeConfig of the root user to use the admin config of the Kubernetes cluster

```bash
master$ mkdir -p /root/.kube
master$ cp -i /etc/kubernetes/admin.conf /root/.kube/config
```

The cloud controller manager and the container storage interface require two secrets in the `kube-system` namespace containing access tokens for the Hetzner Cloud API

```bash
master$ cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: Secret
metadata:
  name: hcloud
  namespace: kube-system
stringData:
  token: <hetzner_api_token>
---
apiVersion: v1
kind: Secret
metadata:
  name: hcloud-csi
  namespace: kube-system
stringData:
  token: <hetzner_api_token>
EOF
```

Both services can use the same token, but if you want to be able to revoke them independend from each other, you need to create two tokens.

> To create a Hetzner Cloud API token log in to the web interface, and navigate to your project -> Access -> API tokens and create a new token. You will not be able to fetch the secret key again later on, so don't close the popup before you have copied the token.

Now deploy the Hetzner Cloud controller manager into the cluster

```bash
master$ kubectl apply -f  https://raw.githubusercontent.com/hetznercloud/hcloud-cloud-controller-manager/master/deploy/v1.3.0.yaml
```

*(Optional)* If you have set up the VPN, you should patch the image of the controller manager to use a version, which announces the `kubernetes.io/internal-ip` label on the server as the nodes internal IP address

```bash
master$ kubectl -n kube-system patch deployment hcloud-cloud-controller-manager --type json -p '[{"op":"replace","path":"/spec/template/spec/containers/0/image","value":"cbeneke/hcloud-cloud-controller-manager:v1.3.0-2"}]'
```

> As Hetzner Cloud servers normally do not have an internal IP address the functionality to detect and report an internal IP address to the Kubernetes apiserver is not implemented in the Hetzner Cloud controller. As the chosen IPs for your wireguard network and the name of the network device (or even the use of wireguard) are not guaranteed I opted for the server label as most flexible solution to patch the controller manager. This requires manually (or e.g. using terraform) labeling the nodes though.

Now set up the cluster networking

```bash
master$ kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/a70459be0084506e4ec919aa1c114638878db11b/Documentation/kube-flannel.yml
```
> This tutorial uses flannel, as it is probably the CNI with the lowest maintenance requirements. For other options and comparisions check the [official documentation](https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/#pod-network)

As the Hetzner Cloud controller manager will add a taint to the nodes, the cluster critical pods need to be patched to tolerate these

```bash
master$ kubectl -n kube-system patch daemonset kube-flannel-ds-amd64 --type json -p '[{"op":"add","path":"/spec/template/spec/tolerations/-","value":{"key":"node.cloudprovider.kubernetes.io/uninitialized","value":"true","effect":"NoSchedule"}}]'
master$ kubectl -n kube-system patch deployment coredns --type json -p '[{"op":"add","path":"/spec/template/spec/tolerations/-","value":{"key":"node.cloudprovider.kubernetes.io/uninitialized","value":"true","effect":"NoSchedule"}}]'
```

> A taint is a setting on a node, that only a specific set of pods are allowed to be scheduled on that node (which is defined by the pod tolerations). The controller-manager will add a `node.cloudprovider.kubernetes.io/uninitialized` taint to each node, which is not yet initialized by the controller-manager. But for the initialization to succeed the networking and DNS on the node must be functional. Further information on taints and tolerations can be found in the [documentation](https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/).

Last but not least deploy the Hetzner Cloud Container Storage Interface to the cluster

```bash
master$ kubectl apply -f https://raw.githubusercontent.com/kubernetes/csi-api/release-1.14/pkg/crd/manifests/csidriver.yaml
master$ kubectl apply -f https://raw.githubusercontent.com/kubernetes/csi-api/release-1.14/pkg/crd/manifests/csinodeinfo.yaml
master$ kubectl apply -f https://raw.githubusercontent.com/hetznercloud/csi-driver/master/deploy/kubernetes/hcloud-csi.yml
```
Your control plane is now ready to use. Fetch the kubeconfig from the master server to be able to use `kubectl` locally

```bash
local$ scp root@<116.203.0.1>:/etc/kubernetes/admin.conf ${HOME}/.kube/config
```

Or merge your existing kubeConfig with the `admin.conf` accordingly.

### Step 3.5 - Join worker nodes

In the `kubeadm init` process a join command for the worker nodes was printed. If you don't have that command noted anymore, a new one can be generated by running the following command on the master node

```bash
master$ kubeadm token create --print-join-command
```

Then log on to each worker node and execute the join command

```bash
worker$ kubeadm join <116.203.0.1>:6443 --token <token> --discovery-token-ca-cert-hash sha256:<hash>
```

When the join was successful list all nodes

```bash
local$ kubectl get nodes
NAME       STATUS   ROLES    AGE   VERSION
master-1   Ready    master   11m   v1.14.1
worker-1   Ready    <none>   5m    v1.14.1
worker-2   Ready    <none>   5m    v1.14.1
```

### Step 3.4 - Setup LoadBalancing (Optional)

Hetzner Cloud does not support LoadBalancer as a Service (yet). Thus [MetalLB](https://metallb.universe.tf/) will be installed to make the LoadBalancer service type available in the cluster.

> A Kubernetes LoadBalancer is typically managed by the cloud controller, but it is not implemented in the hcloud cloud controller (because its not supported by Hetzner Cloud). MetalLB is a project, which provides the LoadBalancer type for baremetal Kubernetes clusters. It announces changes of the IP address endpoint to neighbor-routers, but we will just make use of the LoadBalancer provision in the cluster.

If you have not initialized helm yet run the following commands

```bash
local$ cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: ServiceAccount
metadata:
  name: tiller
  namespace: kube-system
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: tiller
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
  - kind: ServiceAccount
    name: tiller
    namespace: kube-system
EOF
local$ helm init --service-account tiller --history-max 200
```

Now you can install MetalLB with helm

```bash
local$ kubectl create namespace metallb
local$ helm install --name metallb --namespace metallb stable/metallb
```

Then create the config for MetalLB

```bash
local$ cat <<EOF |kubectl apply -f-
apiVersion: v1
kind: ConfigMap
metadata:
  namespace: metallb
  name: metallb-config
data:
  config: |
    address-pools:
    - name: default
      protocol: layer2
      addresses:
      - <159.69.0.1>/32
EOF
```

This will configure MetalLB to use the IPv4 floating IP as LoadBalancer IP. MetalLB can reuse IPs for multiple LoadBalancer services if some [conditions](https://metallb.universe.tf/usage/#ip-address-sharing) are met. This can be enabled by adding an annotation `metallb.universe.tf/allow-shared-ip` to the service.

### Step 3.5 - Setup floating IP failover (Optional)

As the floating IP is bound to one server only I wrote a little controller, which will run in the cluster and reassign the floating IP to another server, if the currently assigned node becomes NotReady.

> If you do not ensure, that the floating IP is always associated to a node in status Ready your cluster will not be high available, as the traffic can be routed to a (potentially) broken node.  I'm currently working on a clone of MetalLB to support Hetzner Cloud API calls when failing over the IP leader, which would make this extra step obsolete. You can check the repo out [here](https://github.com/cbeneke/hcloud-kube-lb) if you are interested in the progress.

To deploy the [Hetzner Cloud floating IP controller](https://github.com/cbeneke/hcloud-fip-controller) create the following resources

```bash
local$ kubectl create namespace fip-controller
local$ kubectl apply -f https://raw.githubusercontent.com/cbeneke/hcloud-fip-controller/master/deploy/rbac.yaml
local$ kubectl apply -f https://raw.githubusercontent.com/cbeneke/hcloud-fip-controller/master/deploy/deployment.yaml
local$ cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: ConfigMap
metadata:
  name: fip-controller-config
  namespace: fip-controller
data:
  config: |
    {
      "floatingIPAddress": "<116.203.0.1>",
      "nodeAddressType": "external"
    }
---
apiVersion: v1
kind: Secret
metadata:
  name: hcloud
  namespace: fip-controller
stringData:
  token: <hetzner_api_token>
EOF
```

> If you did not set up the hcloud cloud controller, the external IP of the nodes might be announced as internalIP of the nodes in the Kubernetes cluster. In that event you must change `nodeAddressType` in the config to `internal` for the floating IP controller to work correctly.

Please be aware, that the project is still in development and the config might be changed drastically in the future. Refer to the [GitHub repository](https://github.com/cbeneke/hcloud-fip-controller) for config options etc.


## Conclusion

Congratulations, you now got a fully featured Kubernetes cluster running on Hetzner Cloud servers. Now you should start to install your first applications on the
cluster or look into [high availability for your control plane](https://kubernetes.io/docs/setup/independent/high-availability/).

##### License: MIT

<!---

Contributors's Certificate of Origin

By making a contribution to this project, I certify that:

(a) The contribution was created in whole or in part by me and I have
    the right to submit it under the license indicated in the file; or

(b) The contribution is based upon previous work that, to the best of my
    knowledge, is covered under an appropriate license and I have the
    right under that license to submit that work with modifications,
    whether created in whole or in part by me, under the same license
    (unless I am permitted to submit under a different license), as
    indicated in the file; or

(c) The contribution was provided directly to me by some other person
    who certified (a), (b) or (c) and I have not modified it.

(d) I understand and agree that this project and the contribution are
    public and that a record of the contribution (including all personal
##     information I submit with it, including my sign-off) is maintained
    indefinitely and may be redistributed consistent with this project
    or the license(s) involved.

Signed-off-by: Christian Beneke <c.beneke@wirelab.org>

-->
